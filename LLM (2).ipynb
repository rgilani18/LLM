{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRBojRT865aQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(\"GPU Available:\", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model and Tokenizer Initialization"
      ],
      "metadata": {
        "id": "KnuvKffGKP0k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"google/flan-t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model = model.to(\"cuda\")  # Move to GPU\n"
      ],
      "metadata": {
        "id": "1vV5oKyJ77mN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first part of the notebook handles the loading of the Large Language Model and its tokenizer from the Hugging Face library."
      ],
      "metadata": {
        "id": "ydibsiaaLfTU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Generation Examples"
      ],
      "metadata": {
        "id": "Gc-lFgRdKmNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This section demonstrates how to format prompts and configure the model's generation parameters."
      ],
      "metadata": {
        "id": "vLSE4vxzLoO4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain gravity in one sentence\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"LLM Response:\", response)  # Output: The capital of France is Paris."
      ],
      "metadata": {
        "id": "s3YXWMjg8CgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "ej0WJCsl8NS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of Pakistan\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"LLM Response:\", response)"
      ],
      "metadata": {
        "id": "S4lgKcgp8dTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "SE-8l0N38X3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"Tell me about California\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"LLM Response:\", response)"
      ],
      "metadata": {
        "id": "Rpts-J4D-ZgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"KoboldAI/fairseq-dense-125M\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"KoboldAI/fairseq-dense-125M\")\n",
        "model.to(\"cuda\")"
      ],
      "metadata": {
        "id": "ZbSzfYzK-nL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt =\"What is the capital of France\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"LLM Response:\", response)"
      ],
      "metadata": {
        "id": "ftqTW_EeACWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the weather in Karachi today?\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=50)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"LLM Response:\", response)"
      ],
      "metadata": {
        "id": "AML-nOnzAHtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Instruction: Explain gravity in one clear sentence.\\nAnswer:\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_length=50,\n",
        "    do_sample=True,   # allows randomness\n",
        "    top_k=50,\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(\"LLM Response:\", response)\n",
        "\n"
      ],
      "metadata": {
        "id": "v0JuzXNPArgE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}